{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%writefile mine_wordnet.py\n",
    "# from nltk.corpus import wordnet as wn\n",
    "# from itertools import product,chain\n",
    "# from collections import defaultdict\n",
    "# import operator,json,csv,sys\n",
    "\n",
    "# def get_definitions(word):\n",
    "#     '''\n",
    "#     Takes a word as input and returns all definitions in the form of a\n",
    "#     dictionary with the synset as keys and definition text as values.\n",
    "#     '''\n",
    "#     definitions = defaultdict()\n",
    "#     for i,j in enumerate(wn.synsets(word)):\n",
    "#         definitions[j] = j.definition()\n",
    "#     return definitions\n",
    "\n",
    "# def get_ontology(seed_syn,word,write_file=False):\n",
    "#     '''\n",
    "#     Takes word as input and returns two dictionaries. One with the list\n",
    "#     of hypernym synsets and one with a list of hyponym synsets.\n",
    "#     '''\n",
    "#     hypernyms = defaultdict()\n",
    "#     hyponyms = defaultdict()\n",
    "#     for i,j in enumerate(wn.synsets(word)):\n",
    "#         hypernyms[j] = \",\".join(list(chain(*[l.lemma_names() for l in j.hypernyms()])))\n",
    "#         hyponyms[j] = \",\".join(list(chain(*[l.lemma_names() for l in j.hyponyms()])))\n",
    "\n",
    "#     hypo_words = [hyponyms.values()[i].split(',') for i in range(len(hyponyms.values())) if hyponyms.values()[i]][0]\n",
    "#     hypo_syns = [get_definitions(hypo_words[i]).keys()[0] for i in xrange(len(hypo_words))]\n",
    "#     relevant_hypo_synsets = calc_seed_similarity(seed_syn,hypo_syns)\n",
    "    \n",
    "#     # Write to file as feeder system to wolfram alpha api code\n",
    "#     if write_file:\n",
    "#         file_endpoint = \"data/\"+word+\"_hyponyms.csv\"\n",
    "#         hypo_out = [str(relevant_hypo_synsets[i][0].lemma_names()[0]).replace('_',' ') for i in range(len(relevant_hypo_synsets))]        \n",
    "#         print(hypo_out)         \n",
    "# #             if i == 0:\n",
    "# #                 with open(file_endpoint, 'w') as data_out:\n",
    "# #                     csv.writer(hypo_out, data_out)\n",
    "# #             else:\n",
    "# #                 with open(file_endpoint, 'a+') as data_out:\n",
    "# #                     csv.writer(hypo_out, data_out)\n",
    "                \n",
    "#         with open(file_endpoint, \"w\") as output:\n",
    "#             writer = csv.writer(output, lineterminator='\\n')\n",
    "# #             for hypo in hypo_out:\n",
    "#             writer.writerow(hypo_out) \n",
    "    \n",
    "#     return hypernyms, relevant_hypo_synsets #hyponyms\n",
    "\n",
    "# def calc_seed_similarity(seed_syn,topic_syns,threshold=.2):\n",
    "#     '''\n",
    "#     Using Wu-Palmer Similiarity compare a seed synset to a list of\n",
    "#     topic synsets. If the similarity score is above the threshold\n",
    "#     it is considered relevant and returned.\n",
    "#     '''\n",
    "#     similarity_scores = defaultdict()\n",
    "#     for i in range(len(topic_syns)):\n",
    "#         score = seed_syn.wup_similarity(topic_syns[i]) # Wu-Palmer Similarity\n",
    "#         if score > threshold:\n",
    "#             similarity_scores[topic_syns[i]] = score\n",
    "#     return sorted(similarity_scores.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "# def create_flashcard(card_front,card_back,mode='w'):\n",
    "#     '''\n",
    "#     Create a flashcard with the front and back of the card as input\n",
    "#     arguments. By default a new data file will be written, but the mode\n",
    "#     can be set to 'a' or 'a+' to append to an existing file.\n",
    "#     '''\n",
    "#     flashcard = {\"fcid\": 1,\n",
    "#                  \"order\": 0,\n",
    "#                  \"term\": card_front,\n",
    "#                  \"definition\": card_back,\n",
    "#                  \"hint\": \"\",\n",
    "#                  \"example\": \"\",\n",
    "#                  \"term_image\": None,\n",
    "#                  \"hint_image\": None}\n",
    "    \n",
    "#     # Save the data\n",
    "#     file_endpoint = \"data/\"+card_front+\".json\"\n",
    "\n",
    "#     if mode.startswith('a'):\n",
    "#         with open(file_endpoint, 'a+') as data_out:\n",
    "#             json.dump(',', data_out)\n",
    "#             json.dump(flashcard, data_out)\n",
    "#     else:\n",
    "#         with open(file_endpoint, 'w') as data_out:\n",
    "#             json.dump(flashcard, data_out) \n",
    "\n",
    "# #### TODO: Not yet converted to real function\n",
    "# # def get_synonyms(word):\n",
    "# #     print \"THESAURUS\"\n",
    "# #     print 50 * '*'\n",
    "# #     for i,j in enumerate(wn.synsets(topic)):\n",
    "# #         print \"Meaning\",i, \"NLTK ID:\", j.name()\n",
    "# #         print \"Definition:\",j.definition()\n",
    "# #         print \"Synonyms:\",  \", \".join(j.lemma_names())\n",
    "# #         print\n",
    "\n",
    "# # Start by providing seed word to filter out unrelated topics/words\n",
    "# SEED = 'math'\n",
    "# seed_syn = get_definitions(SEED).keys()[0]\n",
    "# # print 'SEED: ',SEED\n",
    "# # print seed_syn\n",
    "\n",
    "# topic = 'triangle'\n",
    "# topic_syns = get_definitions(topic)\n",
    "# # print 'TOPIC: ',topic\n",
    "# # print 'SYNSETS: ',topic_syns\n",
    "\n",
    "# relevant_synsets = calc_seed_similarity(seed_syn,topic_syns.keys())\n",
    "# # for i in range(len(relevant_synsets)):\n",
    "# #     create_flashcard(topic,topic_syns[relevant_synsets[i][0]],mode='a+')\n",
    "\n",
    "# hypers,hypos = get_ontology(seed_syn,topic,write_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oblique triangle', 'right triangle', 'obtuse triangle', 'acute triangle', 'wedge', 'equilateral triangle', 'scalene triangle', 'isosceles triangle']\r\n"
     ]
    }
   ],
   "source": [
    "!python mine_wordnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oblique triangle\n",
      "right triangle\n",
      "obtuse triangle\n",
      "acute triangle\n",
      "wedge\n",
      "equilateral triangle\n",
      "scalene triangle\n",
      "isosceles triangle\n"
     ]
    }
   ],
   "source": [
    "hypo_out = ['oblique triangle', 'right triangle', 'obtuse triangle', 'acute triangle', 'wedge', 'equilateral triangle', 'scalene triangle', 'isosceles triangle']\n",
    "\n",
    "for hypo in hypo_out:\n",
    "    print(hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_flashcards.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_flashcards.py\n",
    "\n",
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/<category>\")\n",
    "def get_flashcards(category):\n",
    "    import subprocess\n",
    "    p = subprocess.Popen(\"python mine_wordnet.py \" + category, stdout=subprocess.PIPE, shell=True)\n",
    "    (output, err) = p.communicate()\n",
    "    print output\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product,chain\n",
    "from collections import defaultdict\n",
    "from get_data_from_wolfram import *\n",
    "import get_data_from_wolfram as gdfw\n",
    "import operator,json,csv,sys,subprocess\n",
    "\n",
    "def get_definitions(word):\n",
    "    '''\n",
    "    Takes a word as input and returns all definitions in the form of a\n",
    "    dictionary with the synset as keys and definition text as values.\n",
    "    '''\n",
    "    definitions = defaultdict()\n",
    "    for i,j in enumerate(wn.synsets(word)):\n",
    "        definitions[j] = j.definition()\n",
    "    return definitions\n",
    "\n",
    "def get_ontology(seed_syn,word,write_file=False):\n",
    "    '''\n",
    "    Takes word as input and returns two dictionaries. One with the list\n",
    "    of hypernym synsets and one with a list of hyponym synsets.\n",
    "    '''\n",
    "    hypernyms = defaultdict()\n",
    "    hyponyms = defaultdict()\n",
    "    for i,j in enumerate(wn.synsets(word)):\n",
    "        hypernyms[j] = \",\".join(list(chain(*[l.lemma_names() for l in j.hypernyms()])))\n",
    "        hyponyms[j] = \",\".join(list(chain(*[l.lemma_names() for l in j.hyponyms()])))\n",
    "\n",
    "    if not hyponyms.values():\n",
    "        return [],[]\n",
    "    \n",
    "    hypo_words = [hyponyms.values()[i].split(',') for i in range(len(hyponyms.values())) if hyponyms.values()[i]][0]\n",
    "    hypo_syns = [get_definitions(hypo_words[i].replace(' ','_')).keys()[0] for i in xrange(len(hypo_words))]\n",
    "    relevant_hypo_synsets = calc_seed_similarity(seed_syn,hypo_syns)\n",
    "    \n",
    "    print hypo_syns,relevant_hypo_synsets\n",
    "    \n",
    "    # Write to file as feeder system to wolfram alpha api code\n",
    "    if write_file:\n",
    "        file_endpoint = \"data/\"+word+\"_hyponyms.csv\"\n",
    "        hypo_out = [str(relevant_hypo_synsets[i][0].lemma_names()[0]).replace('_',' ') for i in range(len(relevant_hypo_synsets))]        \n",
    "        # print(hypo_out)         \n",
    "                \n",
    "        with open(file_endpoint, \"w\") as output:\n",
    "            writer = csv.writer(output, lineterminator='\\n')\n",
    "            writer.writerow(hypo_out) \n",
    "    \n",
    "    return hypernyms, relevant_hypo_synsets\n",
    "\n",
    "def calc_seed_similarity(seed_syn,topic_syns,threshold=.2):\n",
    "    '''\n",
    "    Using Wu-Palmer Similiarity compare a seed synset to a list of\n",
    "    topic synsets. If the similarity score is above the threshold\n",
    "    it is considered relevant and returned.\n",
    "    '''\n",
    "    similarity_scores = defaultdict()\n",
    "    for i in range(len(topic_syns)):\n",
    "        score = seed_syn.wup_similarity(topic_syns[i]) # Wu-Palmer Similarity\n",
    "        if score > threshold:\n",
    "            similarity_scores[topic_syns[i]] = score\n",
    "    return sorted(similarity_scores.items(), key=operator.itemgetter(1), reverse=True) \n",
    "\n",
    "def create_flashcard(card_front,card_back,mode='w'):\n",
    "    '''\n",
    "    Create a flashcard with the front and back of the card as input\n",
    "    arguments. By default a new data file will be written, but the mode\n",
    "    can be set to 'a' or 'a+' to append to an existing file.\n",
    "    '''\n",
    "    flashcard = {\"fcid\": 1,\n",
    "                 \"order\": 0,\n",
    "                 \"term\": card_front,\n",
    "                 \"definition\": card_back,\n",
    "                 \"hint\": \"\",\n",
    "                 \"example\": \"\",\n",
    "                 \"term_image\": None,\n",
    "                 \"hint_image\": None}\n",
    "    \n",
    "    # Save the data\n",
    "    file_endpoint = \"data/\"+card_front+\".json\"\n",
    "\n",
    "    if mode.startswith('a'):\n",
    "        with open(file_endpoint, 'a+') as data_out:\n",
    "            json.dump(',', data_out)\n",
    "            json.dump(flashcard, data_out)\n",
    "    else:\n",
    "        with open(file_endpoint, 'w') as data_out:\n",
    "            json.dump(flashcard, data_out) \n",
    "\n",
    "#### TODO: Not yet converted to real function\n",
    "# def get_synonyms(word):\n",
    "#     print \"THESAURUS\"\n",
    "#     print 50 * '*'\n",
    "#     for i,j in enumerate(wn.synsets(topic)):\n",
    "#         print \"Meaning\",i, \"NLTK ID:\", j.name()\n",
    "#         print \"Definition:\",j.definition()\n",
    "#         print \"Synonyms:\",  \", \".join(j.lemma_names())\n",
    "#         print\n",
    "\n",
    "def main(argv):\n",
    "    # Start by providing seed word to filter out unrelated topics/words\n",
    "    SEED = 'math'\n",
    "    seed_syn = get_definitions(SEED).keys()[0]\n",
    "    # print 'SEED: ',SEED\n",
    "    # print seed_syn\n",
    "\n",
    "    topic = argv\n",
    "    topic_syns = get_definitions(topic)\n",
    "    print 'TOPIC: ',topic\n",
    "    # print 'SYNSETS: ',topic_syns\n",
    "\n",
    "    # clean this code: \n",
    "    relevant_synsets = calc_seed_similarity(seed_syn,topic_syns.keys())\n",
    "    print 'REL SYNSETS: ',relevant_synsets\n",
    "    # for i in range(len(relevant_synsets)):\n",
    "    #     create_flashcard(topic,topic_syns[relevant_synsets[i][0]],mode='a+')\n",
    "\n",
    "    _,_ = get_ontology(seed_syn,topic,write_file=True) # hypernyms,hyponyms\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _,topic = sys.argv\n",
    "    main(topic)\n",
    "   \n",
    "    p = subprocess.Popen(\"python get_data_from_wolfram.py \" + topic, stdout=subprocess.PIPE, shell=True)\n",
    "    (output, err) = p.communicate()\n",
    "    print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC:  trigonometry\n",
      "REL SYNSETS:  [(Synset('trigonometry.n.01'), 0.9)]\n",
      "[Synset('spherical_trigonometry.n.01'), Synset('triangulation.n.02')] [(Synset('spherical_trigonometry.n.01'), 0.8571428571428571), (Synset('triangulation.n.02'), 0.3333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "    # Start by providing seed word to filter out unrelated topics/words\n",
    "    SEED = 'math'\n",
    "    seed_syn = get_definitions(SEED).keys()[0]\n",
    "    # print 'SEED: ',SEED\n",
    "    # print seed_syn\n",
    "\n",
    "    topic = \"trigonometry\"\n",
    "    topic_syns = get_definitions(topic)\n",
    "    print 'TOPIC: ',topic\n",
    "    # print 'SYNSETS: ',topic_syns\n",
    "\n",
    "    # clean this code: \n",
    "    relevant_synsets = calc_seed_similarity(seed_syn,topic_syns.keys())\n",
    "    print 'REL SYNSETS: ',relevant_synsets\n",
    "    # for i in range(len(relevant_synsets)):\n",
    "    #     create_flashcard(topic,topic_syns[relevant_synsets[i][0]],mode='a+')\n",
    "\n",
    "    _,_ = get_ontology(seed_syn,topic,write_file=True) # hypernyms,hyponyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mathematics 117, Trigonometry, helps students develop skills sufficiently to write and\\nuse the definition of trigonometric functions; sketch the graph of the trigonometric\\nfunctions; prove identities; solve trigonometric equations; learn and then apply the law\\nof the sines and cosines; learn how to write a complex number in trigonometric form\\nand find all the roots of a complex number; learn polar coordinates system and the\\ngraphs of some simple equations in polar; learn about conic sections (rectangular &\\npolar), vector (applications & operations), and the exponential and logarithmic\\nfunctions with applications and modeling'"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user input\n",
    "syllabus = open(\"/Users/Alexander/hacking_edu/sample_math_syllabus.txt\", \"r\").read()\n",
    "syllabus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This course is intended for college-bound students. In this class, we will study angles,\\ndegree and radian measurements, the six different trigonometric functions, graphing,\\nidentities, solving equations, Law of Sines, Law of Cosines, and other trig. Related\\napplications.\\nChapters to be covered\\n1. The trigonometric functions\\n2. Acute angles and right triangles\\n3. Radian measure and circular functions\\n4. Graphs of circular functions\\n5. Trigonometric identities\\n7. Applications of trigonometry and vectors\\n6. Inverse trigonometric functions and trigonometric equations?'"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user input\n",
    "syllabus_2 = open(\"/Users/Alexander/hacking_edu/syllabus_2.txt\", \"r\").read()\n",
    "syllabus_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_tokens = nltk.tokenize.regexp_tokenize(syllabus, r'[\\w+]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syllabus2_tokens = nltk.tokenize.regexp_tokenize(syllabus_2, r'[\\w+]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter out stop words for syllabus.txt\n",
    "unigrams = [word for word in reg_tokens if word.lower() not in stopwords.words()]\n",
    "bigrams_tuples = [bigram for bigram in nltk.bigrams(tokens)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out stop words for syllabus_2.txt\n",
    "unigrams_2 = [word for word in syllabus2_tokens if word.lower() not in stopwords.words()]\n",
    "bigrams_tuples_2 = [bigram for bigram in nltk.bigrams(syllabus2_tokens)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigrams = [ \" \".join(bigram)  for bigram in bigrams ]\n",
    "bigrams_2 = [ \" \".join(bigram)  for bigram in bigrams_tuples_2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This course',\n",
       " 'course is',\n",
       " 'is intended',\n",
       " 'intended for',\n",
       " 'for college',\n",
       " 'college bound',\n",
       " 'bound students',\n",
       " 'students In',\n",
       " 'In this',\n",
       " 'this class']"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mathematics 117',\n",
       " '117 Trigonometry',\n",
       " 'Trigonometry helps',\n",
       " 'helps students',\n",
       " 'students develop']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mathematics',\n",
       " '117',\n",
       " 'Trigonometry',\n",
       " 'helps',\n",
       " 'students',\n",
       " 'develop',\n",
       " 'skills',\n",
       " 'sufficiently',\n",
       " 'write',\n",
       " 'use']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_lexicon_keywords(lexicon):\n",
    "    lst = []\n",
    "    for keyword in lexicon:\n",
    "        for token in keyword.split(\" \"):\n",
    "            if token not in stopwords.words():\n",
    "                lst.append(token)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_lowercase(math_list):\n",
    "    # lower case all math words \n",
    "    word_list = []\n",
    "    for word in math_list:\n",
    "        try:\n",
    "            word_list.append(word.lower())\n",
    "        except: AttributeError\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_cal = pd.read_csv(\"/Users/Alexander/hacking_edu/calculus_lexicon.csv\", header=None)\n",
    "df_alg = pd.read_csv(\"/Users/Alexander/hacking_edu/algebra_lexicon.csv\", header=None)\n",
    "df_trig = pd.read_csv(\"/Users/Alexander/hacking_edu/trigonometry_lexicon.csv\", header=None)\n",
    "df_geo = pd.read_csv(\"/Users/Alexander/hacking_edu/geometry_lexicon.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cal = df_cal[df_cal.columns].values[0]\n",
    "alg = df_alg[df_alg.columns].values[0]\n",
    "trig = df_trig[df_trig.columns].values[0]\n",
    "geo = df_geo[df_geo.columns].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/kernel/__main__.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "cal  = split_lexicon_keywords(cal)\n",
    "alg  = split_lexicon_keywords(alg)\n",
    "trig = split_lexicon_keywords(trig)\n",
    "geo  = split_lexicon_keywords(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal  = to_lowercase(cal)\n",
    "alg  = to_lowercase(alg)\n",
    "trig = to_lowercase(trig)\n",
    "geo  = to_lowercase(geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# syllabus.txt\n",
    "bigram_keywords = [word for word in bigrams if word in cal or word in alg or word in trig or word in geo]\n",
    "unigram_keywords = [word for word in unigrams if word in cal or word in alg or word in trig or word in geo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# syllabus_2.txt\n",
    "bigram_keywords_2 = [word for word in bigrams_2 if word in cal or word in alg or word in trig or word in geo]\n",
    "unigram_keywords_2 = [word for word in unigrams_2 if word in cal or word in alg or word in trig or word in geo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syllabus.txt\n",
    "bigram_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['definition',\n",
       " 'functions',\n",
       " 'graph',\n",
       " 'functions',\n",
       " 'identities',\n",
       " 'solve',\n",
       " 'equations',\n",
       " 'law',\n",
       " 'sines',\n",
       " 'cosines',\n",
       " 'complex',\n",
       " 'number',\n",
       " 'form',\n",
       " 'complex',\n",
       " 'number',\n",
       " 'polar',\n",
       " 'coordinates',\n",
       " 'system',\n",
       " 'simple',\n",
       " 'equations',\n",
       " 'polar',\n",
       " 'conic',\n",
       " 'sections',\n",
       " 'rectangular',\n",
       " 'polar',\n",
       " 'vector',\n",
       " 'operations',\n",
       " 'exponential',\n",
       " 'logarithmic',\n",
       " 'functions']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syllabus.txt\n",
    "unigram_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syllabus_2.txt\n",
    "bigram_keywords_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bound',\n",
       " 'angles',\n",
       " 'degree',\n",
       " 'radian',\n",
       " 'functions',\n",
       " 'identities',\n",
       " 'equations',\n",
       " 'trig',\n",
       " 'functions',\n",
       " 'angles',\n",
       " 'right',\n",
       " 'triangles',\n",
       " 'measure',\n",
       " 'circular',\n",
       " 'functions',\n",
       " 'circular',\n",
       " 'functions',\n",
       " 'identities',\n",
       " 'trigonometry',\n",
       " 'functions',\n",
       " 'equations']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# syllabus_2.txt\n",
    "unigram_keywords_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#More Signal \n",
    "\n",
    "    The math lexicons are able to pull out more specific math keywords than the wordnet is capable of doing.\n",
    "    We can add a feature to the app -- copy and paste your syllabus math course description and SmartCards will generate flash cards for you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
